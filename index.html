<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Recognition with TensorFlow.js</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection"></script>
    <style>
        body {
            margin: 0;
            overflow: hidden;
        }
        video {
            position: absolute;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            object-fit: cover;
        }
        canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100vw;
            height: 100vh;
            pointer-events: none;
        }
    </style>
</head>
<body>
    <video id="video" autoplay></video>
    <canvas id="canvas"></canvas>
    <script>
        async function setupCamera() {
            const video = document.getElementById('video');
            const constraints = {
                video: {
                    facingMode: 'user',
                    width: { ideal: 1280 },
                    height: { ideal: 720 }
                }
            };
            const stream = await navigator.mediaDevices.getUserMedia(constraints);
            video.srcObject = stream;
            await new Promise((resolve) => {
                video.onloadedmetadata = () => {
                    resolve();
                };
            });
            video.play();
        }

        async function detectFace() {
            const video = document.getElementById('video');
            const canvas = document.getElementById('canvas');
            const ctx = canvas.getContext('2d');

            const model = await faceLandmarksDetection.load(faceLandmarksDetection.SupportedPackages.mediapipeFacemesh);

            // Set canvas size to match video
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;

            async function detect() {
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                const predictions = await model.estimateFaces({
                    input: video
                });

                if (predictions.length > 0) {
                    predictions.forEach(prediction => {
                        // Draw face mesh or landmarks
                        const keypoints = prediction.landmarks;
                        ctx.fillStyle = 'red';
                        keypoints.forEach(point => {
                            ctx.beginPath();
                            ctx.arc(point[0], point[1], 5, 0, 2 * Math.PI);
                            ctx.fill();
                        });
                    });
                }

                requestAnimationFrame(detect);
            }

            detect();
        }

        setupCamera().then(detectFace);
    </script>
</body>
</html>

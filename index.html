<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face Detection with TensorFlow.js (BlazeFace)</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/blazeface"></script>
    <style>
        body { margin: 0; overflow: hidden; }
        video { position: absolute; top: 0; left: 0; width: 100vw; height: 100vh; object-fit: cover; }
        canvas { position: absolute; top: 0; left: 0; width: 100vw; height: 100vh; pointer-events: none; }
    </style>
</head>
<body>
    <video id="video" autoplay></video>
    <canvas id="canvas"></canvas>
    <select id="resolution">
        <option value="640x480">640x480</option>
        <option value="1280x720">1280x720</option>
    </select>
    <button id="flip-camera">Flip Camera</button>
    <script>
        const video = document.getElementById('video');
        const canvas = document.getElementById('canvas');
        const ctx = canvas.getContext('2d');
        const resolutionSelect = document.getElementById('resolution');
        const flipButton = document.getElementById('flip-camera');

        let model, currentStream;
        let isFlipped = false;

        async function setupCamera(resolution) {
            if (currentStream) {
                currentStream.getTracks().forEach(track => track.stop());
            }

            const [width, height] = resolution.split('x').map(Number);
            const constraints = {
                video: { width, height, facingMode: isFlipped ? 'user' : 'environment' },
                audio: false
            };

            currentStream = await navigator.mediaDevices.getUserMedia(constraints);
            video.srcObject = currentStream;

            return new Promise((resolve) => {
                video.onloadedmetadata = () => {
                    resolve(video);
                };
            });
        }

        async function loadModel() {
            return await blazeface.load();
        }

        async function detectFaces() {
            const predictions = await model.estimateFaces(video, false);

            ctx.clearRect(0, 0, canvas.width, canvas.height);

            predictions.forEach(prediction => {
                const start = prediction.topLeft;
                const end = prediction.bottomRight;
                const size = [end[0] - start[0], end[1] - start[1]];

                ctx.save();
                if (isFlipped) {
                    ctx.scale(-1, 1);
                    ctx.translate(-canvas.width, 0);
                }
                ctx.fillStyle = 'rgba(255, 0, 0, 0.5)';
                ctx.fillRect(start[0], start[1], size[0], size[1]);
                ctx.restore();
            });

            requestAnimationFrame(detectFaces);
        }

        function resizeCanvas() {
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
        }

        resolutionSelect.addEventListener('change', async () => {
            await setupCamera(resolutionSelect.value);
            resizeCanvas();
            detectFaces();
        });

        flipButton.addEventListener('click', async () => {
            isFlipped = !isFlipped;
            await setupCamera(resolutionSelect.value);
            resizeCanvas();
            detectFaces();
        });

        window.addEventListener('resize', resizeCanvas);

        async function main() {
            await setupCamera(resolutionSelect.value);
            model = await loadModel();
            resizeCanvas();
            detectFaces();
        }

        main();
    </script>
</body>
</html>
